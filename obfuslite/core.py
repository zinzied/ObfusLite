"""
Core obfuscation engine for ObfusLite
"""

import ast
import base64
import json
import hashlib
import random
from pathlib import Path
from typing import Dict, Any, Optional
from .encoders import get_encoder_class, get_available_techniques

class Obfuscator:
    """
    Main obfuscation class that provides a simple interface for code obfuscation
    """

    def __init__(self):
        self.techniques = {}
        self._register_all_techniques()

    def _register_all_techniques(self):
        """Register all available obfuscation techniques"""
        for technique_name in get_available_techniques():
            encoder_class = get_encoder_class(technique_name)
            if encoder_class:
                self.techniques[technique_name] = encoder_class()

    def obfuscate(self, code: str, technique: str = 'fast_xor',
                  layers: int = 2, seed: Optional[int] = None) -> Dict[str, Any]:
        """
        Obfuscate Python code using specified technique

        Args:
            code: Python source code to obfuscate
            technique: Obfuscation technique to use
            layers: Number of obfuscation layers to apply
            seed: Random seed for reproducible obfuscation

        Returns:
            Dictionary containing obfuscated code and metadata
        """
        if seed is not None:
            random.seed(seed)

        if technique not in self.techniques:
            available = list(self.techniques.keys())
            raise ValueError(f"Unknown technique '{technique}'. Available: {available}")

        # Validate Python syntax
        try:
            ast.parse(code)
        except SyntaxError as e:
            raise ValueError(f"Invalid Python code: {e}")

        # Apply preprocessing
        processed_code = self._preprocess_code(code)

        # Apply multiple layers of obfuscation
        obfuscated_data = processed_code
        layer_metadata = []

        for layer in range(layers):
            encoder = self.techniques[technique]
            result = encoder.encode(obfuscated_data)
            obfuscated_data = result['encoded']
            layer_metadata.append({
                'layer': layer + 1,
                'technique': technique,
                'metadata': result.get('metadata', {})
            })

        # Generate unique identifier
        obfuscation_id = hashlib.sha256(
            f"{technique}_{layers}_{seed}_{len(code)}".encode()
        ).hexdigest()[:16]

        return {
            'obfuscated_code': obfuscated_data,
            'technique': technique,
            'layers': layers,
            'obfuscation_id': obfuscation_id,
            'layer_metadata': layer_metadata,
            'original_size': len(code),
            'obfuscated_size': len(str(obfuscated_data))
        }

    def deobfuscate(self, obfuscated_data: Dict[str, Any]) -> str:
        """
        Deobfuscate code back to original Python source

        Args:
            obfuscated_data: Dictionary containing obfuscated code and metadata

        Returns:
            Original Python source code
        """
        technique = obfuscated_data['technique']
        layers = obfuscated_data['layers']
        layer_metadata = obfuscated_data['layer_metadata']

        if technique not in self.techniques:
            raise ValueError(f"Unknown technique: {technique}")

        # Reverse the layers in reverse order
        decoded_data = obfuscated_data['obfuscated_code']

        for layer in reversed(range(layers)):
            encoder = self.techniques[technique]
            metadata = layer_metadata[layer]['metadata']
            decoded_data = encoder.decode(decoded_data, metadata)

        # Apply postprocessing
        original_code = self._postprocess_code(decoded_data)

        return original_code

    def create_standalone_file(self, obfuscated_data: Dict[str, Any]) -> str:
        """
        Create standalone Python code that includes both the obfuscated data
        and the decoder, suitable for direct execution or PyInstaller compilation

        Args:
            obfuscated_data: Dictionary containing obfuscated code and metadata

        Returns:
            Standalone Python code as string
        """
        technique = obfuscated_data['technique']

        # Convert obfuscated data to JSON string
        obfuscated_json = json.dumps(obfuscated_data, default=str)

        # Get the decoder template based on technique
        decoder_code = self._get_decoder_template(technique)

        # Create the standalone code
        standalone_code = f'''#!/usr/bin/env python3
"""
Obfuscated Python Application
Generated by ObfusLite v1.0.0
This file contains obfuscated code that will be decoded and executed at runtime
"""

import base64
import zlib
import json

def _decode_and_execute():
    """Decode and execute the obfuscated application"""

    # Embedded obfuscated data
    obfuscated_info = {obfuscated_json}

{decoder_code}

    # Decode the layers
    decoded = obfuscated_info['obfuscated_code']
    for layer in reversed(range(obfuscated_info['layers'])):
        metadata = obfuscated_info['layer_metadata'][layer]['metadata']
        decoded = decode_layer(decoded, metadata)

    # Execute the decoded code in global context
    exec(decoded, globals())

if __name__ == "__main__":
    _decode_and_execute()
'''

        return standalone_code

    def _preprocess_code(self, code: str) -> str:
        """Preprocess code before obfuscation"""
        # Remove comments and docstrings while preserving functionality
        try:
            tree = ast.parse(code)

            # Remove docstrings
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):
                    if (node.body and isinstance(node.body[0], ast.Expr) and
                        isinstance(node.body[0].value, ast.Constant) and
                        isinstance(node.body[0].value.value, str)):
                        node.body.pop(0)

            # Convert back to source
            try:
                import astor
                processed = astor.to_source(tree)
            except ImportError:
                # Fallback to original if astor not available
                processed = code
            except Exception:
                # Fallback to original if astor fails
                processed = code

        except SyntaxError:
            # If parsing fails, return original
            processed = code

        return processed.strip()

    def _postprocess_code(self, code: str) -> str:
        """Postprocess code after deobfuscation"""
        return code.strip()

    def _get_decoder_template(self, technique: str) -> str:
        """Get the appropriate decoder template for the technique"""

        if technique == 'fast_xor':
            return '''    def decode_layer(encoded_data, metadata):
        """Fast XOR decoder"""
        keys = metadata['keys']
        compressed = base64.b64decode(encoded_data.encode('ascii'))
        encoded_bytes = bytearray(zlib.decompress(compressed))

        for i, byte in enumerate(encoded_bytes):
            key_index = i % len(keys)
            encoded_bytes[i] = byte ^ keys[key_index]

        return bytes(encoded_bytes).decode('utf-8')'''

        elif technique == 'fast_base64':
            return '''    def decode_layer(encoded_data, metadata):
        """Fast Base64 decoder"""
        char_map = metadata['char_map']
        reverse_map = {v: k for k, v in char_map.items()}
        original_b64 = ''.join(reverse_map.get(c, c) for c in encoded_data)
        decoded_bytes = base64.b64decode(original_b64.encode('ascii'))
        return decoded_bytes.decode('utf-8')'''

        elif technique == 'fast_rotation':
            return '''    def decode_layer(encoded_data, metadata):
        """Fast Rotation decoder"""
        rotations = metadata['rotations']
        encoded = base64.b64decode(encoded_data.encode('ascii')).decode('utf-8')

        for rotation in reversed(rotations):
            result = []
            for char in encoded:
                if char.isalpha():
                    base = ord('A') if char.isupper() else ord('a')
                    rotated = chr((ord(char) - base - rotation) % 26 + base)
                    result.append(rotated)
                else:
                    result.append(char)
            encoded = ''.join(result)

        return encoded'''

        else:
            # Generic decoder for other techniques
            return '''    def decode_layer(encoded_data, metadata):
        """Generic decoder - requires full library"""
        from obfuslite.encoders import get_encoder_class
        encoder_class = get_encoder_class(obfuscated_info['technique'])
        encoder = encoder_class()
        return encoder.decode(encoded_data, metadata)'''

    def get_available_techniques(self) -> list:
        """Get list of available obfuscation techniques"""
        return list(self.techniques.keys())

    def get_technique_info(self, technique: str) -> Dict[str, str]:
        """Get information about a specific technique"""
        info_map = {
            'fast_xor': {'type': 'fast', 'description': 'Multi-key XOR with compression'},
            'fast_base64': {'type': 'fast', 'description': 'Base64 with character substitution'},
            'fast_rotation': {'type': 'fast', 'description': 'Multi-round Caesar cipher'},
            'fast_hash': {'type': 'fast', 'description': 'Hash-based chunk encoding'},
            'fast_binary': {'type': 'fast', 'description': 'Binary manipulation with bit shifting'},
            'fast_lookup': {'type': 'fast', 'description': 'Character lookup table encoding'},
            'simple': {'type': 'fast', 'description': 'Simple XOR encoding'},
            'quantum': {'type': 'advanced', 'description': 'Quantum-inspired encoding'},
            'dna': {'type': 'advanced', 'description': 'DNA sequence mapping'},
            'fractal': {'type': 'advanced', 'description': 'Fractal pattern encoding'},
            'neural': {'type': 'advanced', 'description': 'Neural network weight encoding'},
            'steganographic': {'type': 'advanced', 'description': 'Steganographic hiding'},
            'runtime': {'type': 'advanced', 'description': 'Runtime reconstruction'},
            'tensor': {'type': 'advanced', 'description': 'Multi-dimensional tensor encoding'}
        }

        return info_map.get(technique, {'type': 'unknown', 'description': 'Unknown technique'})


def combine_python_files(main_file: str, output_file: str = "combined_app.py") -> str:
    """
    Combine multiple Python files into a single file with improved dependency analysis

    Args:
        main_file: Path to the main Python file
        output_file: Output file name for combined code

    Returns:
        Path to the combined file
    """
    import importlib.util
    import sys
    import os

    def analyze_imports(file_path):
        """Analyze imports in a Python file and categorize them"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            print(f"âŒ Syntax error in {file_path}: {e}")
            return [], [], content, []

        external_imports = []  # Third-party and standard library
        local_imports = []     # Local modules that will be combined
        other_code = []
        local_dependencies = []  # Track which local modules this file depends on

        # Get list of local module names in the project
        main_dir = Path(file_path).parent
        local_module_names = set()
        local_module_paths = {}

        for py_file in main_dir.rglob("*.py"):
            if py_file.name != "__init__.py":
                module_name = py_file.stem
                local_module_names.add(module_name)
                local_module_paths[module_name] = py_file

        for node in tree.body:
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                skip_import = False
                is_local = False

                if isinstance(node, ast.ImportFrom):
                    if node.module:
                        # Handle relative imports
                        if node.module.startswith('.'):
                            skip_import = True
                            is_local = True
                            # Extract the actual module name from relative import
                            relative_module = node.module.lstrip('.')
                            if relative_module and relative_module in local_module_names:
                                local_dependencies.append(relative_module)
                        # Check if it's a local module
                        elif node.module in local_module_names:
                            skip_import = True
                            is_local = True
                            local_dependencies.append(node.module)
                        # More conservative filtering - only skip if we're sure it's local
                        elif (node.module.split('.')[0] in local_module_names or
                              any(node.module.startswith(f"{local}.") for local in local_module_names)):
                            skip_import = True
                            is_local = True
                            local_dependencies.append(node.module.split('.')[0])

                elif isinstance(node, ast.Import):
                    # Check each imported name
                    for alias in node.names:
                        if alias.name in local_module_names:
                            skip_import = True
                            is_local = True
                            local_dependencies.append(alias.name)
                            break

                if skip_import:
                    if is_local:
                        local_imports.append(ast.unparse(node))
                else:
                    external_imports.append(ast.unparse(node))
            else:
                other_code.append(ast.unparse(node))

        return external_imports, local_imports, '\n'.join(other_code), local_dependencies

    def find_actually_imported_modules(main_file):
        """Find only the modules that are actually imported by analyzing the dependency tree"""
        main_dir = Path(main_file).parent
        visited = set()
        to_process = [Path(main_file)]
        dependency_graph = {}

        while to_process:
            current_file = to_process.pop(0)
            if current_file in visited:
                continue

            visited.add(current_file)

            try:
                external_imports, local_imports, code, local_deps = analyze_imports(current_file)
                dependency_graph[current_file] = {
                    'external_imports': external_imports,
                    'local_imports': local_imports,
                    'code': code,
                    'dependencies': local_deps
                }

                # Add dependencies to processing queue
                for dep in local_deps:
                    dep_file = main_dir / f"{dep}.py"
                    if dep_file.exists() and dep_file not in visited:
                        to_process.append(dep_file)

            except Exception as e:
                print(f"âš ï¸  Warning: Could not analyze {current_file}: {e}")

        return dependency_graph

    def topological_sort(dependency_graph, main_file):
        """Sort files in dependency order (dependencies first)"""
        # Build adjacency list
        graph = {}
        in_degree = {}

        for file_path in dependency_graph:
            graph[file_path] = []
            in_degree[file_path] = 0

        for file_path, data in dependency_graph.items():
            for dep in data['dependencies']:
                dep_file = file_path.parent / f"{dep}.py"
                if dep_file in dependency_graph:
                    graph[dep_file].append(file_path)
                    in_degree[file_path] += 1

        # Topological sort using Kahn's algorithm
        queue = [f for f in in_degree if in_degree[f] == 0]
        result = []

        while queue:
            current = queue.pop(0)
            result.append(current)

            for neighbor in graph[current]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        # Ensure main file is last
        main_path = Path(main_file)
        if main_path in result:
            result.remove(main_path)
        result.append(main_path)

        return result

    print(f"ðŸ” Analyzing dependencies starting from '{main_file}'...")

    # Get dependency graph
    dependency_graph = find_actually_imported_modules(main_file)

    if not dependency_graph:
        print("âŒ No files found or could not analyze dependencies")
        return output_file

    print(f"ðŸ“ Found {len(dependency_graph)} Python files with dependencies:")
    for file_path, data in dependency_graph.items():
        deps = data['dependencies']
        dep_str = f" (depends on: {', '.join(deps)})" if deps else " (no local dependencies)"
        print(f"   - {file_path.name}{dep_str}")

    # Sort files in dependency order
    print("\nðŸ”„ Sorting files by dependencies...")
    sorted_files = topological_sort(dependency_graph, main_file)

    print("ðŸ“‹ Processing order:")
    for i, file_path in enumerate(sorted_files, 1):
        marker = " [MAIN]" if file_path == Path(main_file) else ""
        print(f"   {i}. {file_path.name}{marker}")

    # Combine all files in dependency order
    all_external_imports = set()
    all_code = []
    skipped_local_imports = []

    print("\nðŸ”„ Processing files...")

    for file_path in sorted_files:
        print(f"   Processing: {file_path.name}")
        data = dependency_graph[file_path]

        all_external_imports.update(data['external_imports'])
        skipped_local_imports.extend(data['local_imports'])

        if data['code'].strip():
            is_main = file_path == Path(main_file)
            marker = "Main code" if is_main else "Code"
            all_code.append(f"\n# === {marker} from {file_path.name} ===")
            all_code.append(data['code'])

    # Create combined file
    combined_content = []

    # Add header with more information
    combined_content.append('#!/usr/bin/env python3')
    combined_content.append('"""')
    combined_content.append('Combined Python Application')
    combined_content.append('Generated by ObfusLite Combiner v2.0')
    combined_content.append(f'Source files: {len(sorted_files)} files combined')
    combined_content.append(f'Main file: {Path(main_file).name}')
    if skipped_local_imports:
        combined_content.append(f'Local imports removed: {len(skipped_local_imports)}')
    combined_content.append('"""')
    combined_content.append('')

    # Add all external imports (sorted and deduplicated)
    if all_external_imports:
        combined_content.append('# === External Imports ===')
        combined_content.extend(sorted(all_external_imports))
        combined_content.append('')

    # Add information about skipped local imports as comments
    if skipped_local_imports:
        combined_content.append('# === Removed Local Imports (now combined) ===')
        for local_import in sorted(set(skipped_local_imports)):
            combined_content.append(f'# {local_import}')
        combined_content.append('')

    # Add all code
    combined_content.extend(all_code)

    # Write combined file
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(combined_content))

        # Validate the combined file
        print(f"\nðŸ§ª Validating combined file...")
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                combined_code = f.read()
            compile(combined_code, output_file, 'exec')
            print(f"âœ… Combined file syntax is valid")
        except SyntaxError as e:
            print(f"âš ï¸  Warning: Syntax error in combined file: {e}")
            print(f"   Line {e.lineno}: {e.text}")
            print(f"   You may need to manually fix the combined file")

        print(f"\nâœ… Combined file created: {output_file}")
        print(f"   ðŸ“Š Statistics:")
        print(f"   - Files combined: {len(sorted_files)}")
        print(f"   - External imports: {len(all_external_imports)}")
        print(f"   - Local imports removed: {len(set(skipped_local_imports))}")
        print(f"   - Total lines: {len(combined_content)}")

        return output_file

    except Exception as e:
        print(f"âŒ Error writing combined file: {e}")
        raise


def debug_combined_file(combined_file: str) -> Dict[str, Any]:
    """
    Debug a combined Python file to identify potential issues

    Args:
        combined_file: Path to the combined Python file

    Returns:
        Dictionary with debugging information
    """
    debug_info = {
        'file_exists': False,
        'syntax_valid': False,
        'syntax_error': None,
        'imports': [],
        'functions': [],
        'classes': [],
        'global_vars': [],
        'potential_issues': []
    }

    try:
        # Check if file exists
        if not Path(combined_file).exists():
            debug_info['potential_issues'].append(f"File does not exist: {combined_file}")
            return debug_info

        debug_info['file_exists'] = True

        # Read file content
        with open(combined_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Check syntax
        try:
            tree = ast.parse(content)
            debug_info['syntax_valid'] = True
        except SyntaxError as e:
            debug_info['syntax_error'] = {
                'message': str(e),
                'line': e.lineno,
                'offset': e.offset,
                'text': e.text
            }
            debug_info['potential_issues'].append(f"Syntax error at line {e.lineno}: {e.msg}")
            return debug_info

        # Analyze AST
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    debug_info['imports'].append({
                        'type': 'import',
                        'name': alias.name,
                        'asname': alias.asname
                    })
            elif isinstance(node, ast.ImportFrom):
                for alias in node.names:
                    debug_info['imports'].append({
                        'type': 'from_import',
                        'module': node.module,
                        'name': alias.name,
                        'asname': alias.asname
                    })
            elif isinstance(node, ast.FunctionDef):
                debug_info['functions'].append({
                    'name': node.name,
                    'line': node.lineno,
                    'args': [arg.arg for arg in node.args.args]
                })
            elif isinstance(node, ast.ClassDef):
                debug_info['classes'].append({
                    'name': node.name,
                    'line': node.lineno,
                    'bases': [ast.unparse(base) for base in node.bases]
                })
            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        debug_info['global_vars'].append({
                            'name': target.id,
                            'line': node.lineno
                        })

        # Check for potential issues
        function_names = [f['name'] for f in debug_info['functions']]
        class_names = [c['name'] for c in debug_info['classes']]

        # Check for duplicate function names
        if len(function_names) != len(set(function_names)):
            duplicates = [name for name in set(function_names) if function_names.count(name) > 1]
            debug_info['potential_issues'].append(f"Duplicate function names: {duplicates}")

        # Check for duplicate class names
        if len(class_names) != len(set(class_names)):
            duplicates = [name for name in set(class_names) if class_names.count(name) > 1]
            debug_info['potential_issues'].append(f"Duplicate class names: {duplicates}")

        # Check for missing common imports
        import_names = [imp['name'] for imp in debug_info['imports']]
        if 'sys' not in import_names and '__file__' in content:
            debug_info['potential_issues'].append("Code uses __file__ but 'sys' not imported")

        if 'os' not in import_names and ('os.path' in content or 'os.getcwd' in content):
            debug_info['potential_issues'].append("Code uses os functions but 'os' not imported")

    except Exception as e:
        debug_info['potential_issues'].append(f"Error during analysis: {e}")

    return debug_info
