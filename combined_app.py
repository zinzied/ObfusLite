#!/usr/bin/env python3
"""
Combined Python Application
Generated by ObfusLite Combiner
"""

# === All Imports ===
from colorama import Fore, init
from colorama import init, Fore, Style
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from fake_useragent import UserAgent
from threading import Lock
from tqdm import tqdm
import concurrent.futures
import logging
import os
import pyfiglet
import random
import re
import requests
import socket
import sys
import threading
import time


# === Code from scraper.py ===
init()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.FileHandler('scraper.log'), logging.StreamHandler()])
SCRAPING_THREADS = 30
TESTING_THREADS = 500
TIMEOUT = 5
SAVE_DIRECTORY = os.path.join(os.path.dirname(__file__), 'proxies')
TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')
ANIMATION_ACTIVE = False
ANIMATION_SPEED = 0.05
ANIMATION_LOCK = threading.Lock()
CURRENT_PROXY_INFO = {'type': '', 'count': 0, 'total_scraped': 0, 'mode': 'scraping', 'progress': 0, 'total': 0}
SCRAPING_DELAY = 2.0
VALIDATION_DELAY = 0.05
os.makedirs(SAVE_DIRECTORY, exist_ok=True)
proxy_urls = ['https://api.proxyscrape.com/v3/free-proxy-list/get?request=displayproxies&proxy_format=ipport&format=text&timeout=20000', 'https://github.com/TheSpeedX/PROXY-List/blob/master/socks5.txt', 'https://github.com/TheSpeedX/PROXY-List/blob/master/http.txt', 'https://github.com/TheSpeedX/PROXY-List/blob/master/socks4.txt', 'https://www.proxy-list.download/api/v1/get?type=socks4', 'https://www.proxy-list.download/api/v1/get?type=socks5', 'https://www.proxy-list.download/api/v1/get?type=http', 'https://sunny9577.github.io/proxy-scraper/generated/http_proxies.txt', 'https://sunny9577.github.io/proxy-scraper/generated/socks5_proxies.txt', 'https://sunny9577.github.io/proxy-scraper/generated/socks4_proxies.txt', 'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt', 'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/socks4.txt', 'https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/socks5.txt', 'https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt', 'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt', 'https://raw.githubusercontent.com/monosans/proxy-list/main/proxies_anonymous/http.txt', 'https://raw.githubusercontent.com/hookzof/socks5_list/master/proxy.txt', 'https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt', 'https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks4.txt', 'https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks5.txt', 'https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-http.txt', 'https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-https.txt', 'https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-socks4.txt', 'https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-socks5.txt', 'https://api.openproxylist.xyz/http.txt', 'https://api.openproxylist.xyz/socks4.txt', 'https://api.openproxylist.xyz/socks5.txt', 'https://proxyspace.pro/http.txt', 'https://proxyspace.pro/https.txt', 'https://proxyspace.pro/socks4.txt', 'https://proxyspace.pro/socks5.txt']
def validate_proxy(proxy, test_url='http://www.google.com'):
    """Test if proxy is working with improved validation"""
    try:
        host, port = proxy.split(':')
        with socket.create_connection((host, int(port)), timeout=TIMEOUT):
            return True
    except:
        return False
def classify_proxy(url):
    """Classify proxy type based on URL"""
    if 'socks4' in url.lower():
        return 'socks4'
    elif 'socks5' in url.lower():
        return 'socks5'
    else:
        return 'http'
def update_proxy_info(proxy_type, count, mode='scraping', progress=0, total=0):
    """Update the current proxy info for the animation thread"""
    with ANIMATION_LOCK:
        CURRENT_PROXY_INFO['type'] = proxy_type
        CURRENT_PROXY_INFO['count'] = count
        CURRENT_PROXY_INFO['mode'] = mode
        CURRENT_PROXY_INFO['progress'] = progress
        CURRENT_PROXY_INFO['total'] = total
def run_animation():
    """Run a continuous animation in a separate thread"""
    animation_chars = ['⣾', '⣽', '⣻', '⢿', '⡿', '⣟', '⣯', '⣷']
    flow_patterns = ['▁▂▃▄▅▆▇█▇▆▅▄▃▂▁', '←←←←←←←←', '→→→→→→→→', '▌▀▐▄', '▖▘▝▗', '▉▊▋▌▍▎▏▎▍▌▋▊▉', '▁▃▄▅▆▇█▇▆▅▄▃▁']
    progress_chars = ['▏', '▎', '▍', '▌', '▋', '▊', '▉', '█']
    symbols = ['🔄', '✨', '🔍', '📡', '🌐', '⚡', '💻', '🔎', '📶', '🛰️']
    error_symbols = ['❌', '⚠️', '🚫', '⛔', '🔴']
    colors = [Fore.GREEN, Fore.CYAN, Fore.BLUE, Fore.MAGENTA]
    frame = 0
    symbol_idx = 0
    color_idx = 0
    error_idx = 0
    flow_idx = 0
    flow_pos = 0
    while ANIMATION_ACTIVE:
        with ANIMATION_LOCK:
            proxy_type = CURRENT_PROXY_INFO['type']
            count = CURRENT_PROXY_INFO['count']
            mode = CURRENT_PROXY_INFO['mode']
            progress = CURRENT_PROXY_INFO['progress']
            total = CURRENT_PROXY_INFO['total']
        sys.stdout.write('\r' + ' ' * 120)
        if proxy_type == 'error':
            error_symbol = error_symbols[error_idx]
            flow_char = flow_patterns[flow_idx][flow_pos % len(flow_patterns[flow_idx])]
            sys.stdout.write(f'\r{Fore.RED}{error_symbol} {flow_char} Error scraping a source {flow_char} {error_symbol}')
            error_idx = (error_idx + 1) % len(error_symbols)
        elif mode == 'validating' and total > 0:
            char = animation_chars[frame % len(animation_chars)]
            flow_char = flow_patterns[flow_idx][flow_pos % len(flow_patterns[flow_idx])]
            symbol = symbols[symbol_idx]
            color = colors[color_idx]
            percentage = progress / total * 100 if total > 0 else 0
            progress_bar_length = 20
            filled_length = int(progress_bar_length * progress // total) if total > 0 else 0
            progress_bar = '█' * filled_length + '░' * (progress_bar_length - filled_length)
            sys.stdout.write(f'\r{color}{char} {flow_char} {symbol} Validating {proxy_type} proxies: [{progress_bar}] {percentage:.1f}% ({progress}/{total}) {symbol} {flow_char} {char}')
        elif proxy_type and count > 0:
            char = animation_chars[frame % len(animation_chars)]
            flow_char = flow_patterns[flow_idx][flow_pos % len(flow_patterns[flow_idx])]
            symbol = symbols[symbol_idx]
            color = colors[color_idx]
            sys.stdout.write(f'\r{color}{char} {flow_char} {symbol} Successfully scraped {count} {proxy_type} proxies {symbol} {flow_char} {char}')
        frame = (frame + 1) % len(animation_chars)
        flow_pos = (flow_pos + 1) % len(flow_patterns[flow_idx])
        if frame % 3 == 0:
            symbol_idx = (symbol_idx + 1) % len(symbols)
        if frame % 7 == 0:
            color_idx = (color_idx + 1) % len(colors)
        if frame % 15 == 0:
            flow_idx = (flow_idx + 1) % len(flow_patterns)
        sys.stdout.flush()
        time.sleep(ANIMATION_SPEED)
def display_scraping_animation(proxy_type, count):
    """Update the proxy info for the animation thread for scraping mode"""
    update_proxy_info(proxy_type, count, mode='scraping')
def display_validation_animation(proxy_type, progress, total):
    """Update the proxy info for the animation thread for validation mode"""
    update_proxy_info(proxy_type, 0, mode='validating', progress=progress, total=total)
def scrape_proxies_parallel(urls):
    """Scrape proxies using parallel processing"""

    def scrape_single_url(url):
        try:
            proxy_type = classify_proxy(url)
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                if 'proxyscrape' in url:
                    proxies = response.text.split('\n')
                else:
                    proxies = re.findall('(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}:\\d{1,5})', response.text)
                return (proxy_type, proxies, True, None)
            return (proxy_type, [], False, f'Status: {response.status_code}')
        except Exception as e:
            return (proxy_type, [], False, str(e))
    results = {}
    with ThreadPoolExecutor(max_workers=SCRAPING_THREADS) as executor:
        futures = {executor.submit(scrape_single_url, url): url for url in urls}
        for future in tqdm(futures, desc=f'Scraping proxies ({SCRAPING_THREADS} threads)'):
            url = futures[future]
            proxy_type, proxies, success, error = future.result()
            if success:
                display_scraping_animation(proxy_type, len(proxies))
                if proxy_type not in results:
                    results[proxy_type] = set()
                results[proxy_type].update(proxies)
                time.sleep(SCRAPING_DELAY)
            else:
                update_proxy_info('error', 0)
                time.sleep(1.0)
    return results
def main():
    """Main execution function"""
    global ANIMATION_ACTIVE
    try:
        print(f'{Fore.CYAN}Starting proxy scraper with {SCRAPING_THREADS} scraping threads and {TESTING_THREADS} testing threads')
        ANIMATION_ACTIVE = True
        animation_thread = threading.Thread(target=run_animation, daemon=True)
        animation_thread.start()
        classified_proxies = scrape_proxies_parallel(proxy_urls)
        ANIMATION_ACTIVE = False
        animation_thread.join(timeout=1.0)
        sys.stdout.write('\r' + ' ' * 100)
        sys.stdout.write('\r')
        sys.stdout.flush()
        if not any(classified_proxies.values()):
            logging.error('No proxies were scraped. Exiting.')
            return
        print(f'\n{Fore.YELLOW}Validating proxies...')
        valid_proxies = {'http': set(), 'socks4': set(), 'socks5': set()}
        ANIMATION_ACTIVE = True
        animation_thread = threading.Thread(target=run_animation, daemon=True)
        animation_thread.start()
        for proxy_type, proxies in classified_proxies.items():
            proxies_list = list(proxies)
            total_proxies = len(proxies_list)
            valid_count = 0
            display_validation_animation(proxy_type, 0, total_proxies)
            with ThreadPoolExecutor(max_workers=TESTING_THREADS) as executor:
                futures = {executor.submit(validate_proxy, proxy): i for i, proxy in enumerate(proxies_list)}
                completed_count = 0
                update_frequency = 50
                for i, future in enumerate(concurrent.futures.as_completed(futures)):
                    proxy_index = futures[future]
                    proxy = proxies_list[proxy_index]
                    is_valid = future.result()
                    completed_count += 1
                    if is_valid:
                        valid_proxies[proxy_type].add(proxy)
                        valid_count += 1
                    if completed_count % update_frequency == 0 or completed_count == total_proxies:
                        display_validation_animation(proxy_type, completed_count, total_proxies)
                        time.sleep(VALIDATION_DELAY)
            print()
        ANIMATION_ACTIVE = False
        animation_thread.join(timeout=1.0)
        sys.stdout.write('\r' + ' ' * 120)
        sys.stdout.write('\r')
        sys.stdout.flush()
        for proxy_type, proxies in valid_proxies.items():
            if proxies:
                timestamp_filename = f'{proxy_type}_proxies_{TIMESTAMP}.txt'
                timestamp_filepath = os.path.join(SAVE_DIRECTORY, timestamp_filename)
                with open(timestamp_filepath, 'w') as file:
                    for proxy in proxies:
                        file.write(f'{proxy}\n')
                standard_filename = f'{proxy_type}-valid.txt'
                standard_filepath = os.path.join(SAVE_DIRECTORY, standard_filename)
                with open(standard_filepath, 'w') as file:
                    for proxy in proxies:
                        file.write(f'{proxy}\n')
                print(f'{Fore.GREEN}Saved {len(proxies)} {proxy_type} proxies to:')
                print(f'{Fore.GREEN}- {timestamp_filepath} (archived)')
                print(f'{Fore.GREEN}- {standard_filepath} (for easy access)')
        print(f'\n{Fore.CYAN}=== Proxy Scraping Results ===')
        for proxy_type in valid_proxies:
            total = len(classified_proxies.get(proxy_type, set()))
            valid = len(valid_proxies[proxy_type])
            success_rate = valid / total * 100 if total > 0 else 0
            print(f'{Fore.WHITE}{proxy_type.upper()}:')
            print(f'Total scraped: {total}')
            print(f'Valid proxies: {valid}')
            print(f'Success rate: {success_rate:.2f}%\n')
    except Exception as e:
        logging.error(f'An unexpected error occurred: {e}')
if __name__ == '__main__':
    main()

# === Main code from main.py ===
FONTS = ['slant', 'banner3', 'cosmic', 'digital', 'doom', 'big', 'block', 'roman', 'shadow', 'standard', 'starwars', 'stop', 'straight', 'weird']
init()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
file_lock = Lock()
def print_welcome_message():
    logo = '\n██████╗ ██████╗  ██████╗ ██╗  ██╗██╗   ██╗██████╗  ██████╗ ██╗     ██╗     \n██╔══██╗██╔══██╗██╔═══██╗╚██╗██╔╝╚██╗ ██╔╝██╔══██╗██╔═══██╗██║     ██║     \n██████╔╝██████╔╝██║   ██║ ╚███╔╝  ╚████╔╝ ██████╔╝██║   ██║██║     ██║     \n██╔═══╝ ██╔══██╗██║   ██║ ██╔██╗   ╚██╔╝  ██╔══██╗██║   ██║██║     ██║     \n██║     ██║  ██║╚██████╔╝██╔╝ ██╗   ██║   ██║  ██║╚██████╔╝███████╗███████╗\n╚═╝     ╚═╝  ╚═╝ ╚═════╝ ╚═╝  ╚═╝   ╚═╝   ╚═╝  ╚═════╝ ╚══════╝╚══════╝\n                                                                                                                                                             \n    '
    print(Fore.CYAN + Style.BRIGHT + '=' * 50)
    print(Fore.CYAN + Style.BRIGHT + '     Proxy Testing Script     ')
    print(Fore.GREEN + Style.BRIGHT + logo)
    random_font = random.choice(FONTS)
    try:
        ascii_name = pyfiglet.figlet_format('BY ZIEDEV 2025', font=random_font)
    except Exception:
        ascii_name = pyfiglet.figlet_format('BY ZIEDEV 2025', font='standard')
    print(Fore.YELLOW + Style.BRIGHT + ascii_name)
    print(Fore.CYAN + Style.BRIGHT + f'Font: {random_font}')
    print(Fore.CYAN + Style.BRIGHT + '=============================')
    print(Fore.CYAN + Style.BRIGHT + ' Enjoy the Proxies Checker!! ')
    print(Fore.CYAN + Style.BRIGHT + '=' * 50)
    print(Style.RESET_ALL)
def read_proxies_from_file(filename):
    try:
        with open(filename, 'r') as file:
            proxies = file.readlines()
        normalized_proxies = []
        for proxy in proxies:
            proxy = proxy.strip()
            proxy = re.sub('^socks4://|^socks5://|^http://|^https://', '', proxy)
            normalized_proxies.append(proxy)
        return normalized_proxies
    except FileNotFoundError:
        logging.error(f'File {filename} not found.')
        return []
def get_random_user_agent():
    user_agent = UserAgent()
    return user_agent.random
def determine_proxy_type(proxy):
    if ':80' in proxy or ':443' in proxy:
        return 'http'
    if ':1080' in proxy or ':1081' in proxy:
        return 'socks5'
    if ':5210' in proxy:
        return 'socks4'
    return 'unknown'
def check_proxy(proxy, valid_proxies):
    user_agent = get_random_user_agent()
    headers = {'User-Agent': user_agent}
    proxy_type = determine_proxy_type(proxy)
    proxy_dict = {'http': proxy if proxy_type == 'http' else None, 'https': proxy if proxy_type == 'http' else None}
    retries = 100
    timeout = 5
    for attempt in range(retries):
        try:
            url = 'http://httpbin.org/get'
            r = requests.get(url, proxies=proxy_dict, headers=headers, timeout=timeout)
            if r.status_code == 200:
                filename = {'http': 'http-valid.txt', 'socks4': 'socks4-valid.txt', 'socks5': 'socks5-valid.txt'}.get(proxy_type, None)
                if filename:
                    print(Fore.GREEN + f'Proxy {proxy} is valid.' + Style.RESET_ALL)
                    valid_proxies.append(proxy)
                    with file_lock:
                        with open(filename, 'a') as file:
                            file.write(proxy + '\n')
                return
        except requests.exceptions.Timeout:
            logging.warning(f'Timeout occurred for proxy {proxy} on attempt {attempt + 1}. Retrying...')
        except requests.exceptions.ConnectionError:
            logging.error(f'Connection error for proxy {proxy}.')
            break
        except requests.exceptions.RequestException as e:
            logging.error(f'Request exception for proxy {proxy}: {str(e)}')
        time.sleep(2 ** attempt)
        timeout *= 2
    print(Fore.RED + f'Proxy {proxy} is not valid.' + Style.RESET_ALL)
def main():
    print_welcome_message()
    try:
        print(f'{Fore.CYAN}Starting proxy scraper...{Style.RESET_ALL}')
        scrape_proxies()
        print(f"{Fore.GREEN}Proxy scraping completed! Check the 'proxies' folder for results.{Style.RESET_ALL}")
    except KeyboardInterrupt:
        print(f'\n{Fore.YELLOW}Scraping interrupted by user.{Style.RESET_ALL}')
        sys.exit(1)
    except Exception as e:
        print(f'{Fore.RED}An error occurred: {str(e)}{Style.RESET_ALL}')
        sys.exit(1)
if __name__ == '__main__':
    main()